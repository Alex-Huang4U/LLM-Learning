# 语言模型发展历程（4个阶段）
#语言模型的发展历程
## 统计语言模型（Statistical Language Model, SLM）

### 马尔可夫假设（Markov Assumption）

- 根据词序列中==若干个连续的上下文单词==来预测下一个词的出现概率，即根据一个==固定长度的前缀==来预测目标单词。

-  具有固定上下文长度 𝑛 的统计语言模型通常被称为 𝑛 元 （𝑛-gram）语言模型

- “维数灾难”（Curse of Dimensionality）：对于高阶统计语言模型来说，随着阶数 𝑛 的增加，需要估计的==转移概率项数==将会指数级增长
### 退估计（Back-off Estimation）和古德-图灵估计（Good-Turing Estimation

- 为了缓解数据稀疏问题，所设计专门的语言模型平滑策略

- 对于高阶上下文的刻画能力仍然较弱，无法精确建模复杂的高阶语义关系

## 神经语言模型（Neural Language Model, NLM）

## 预训练语言模型（Pre-trained Language Model, PLM）

- 预训练语言模型在训练架构与训练数据两个方面进行了改进与创新

- ELMo [11] 是一个早期的代表性预训练语言模型，提出使用大量的无标注数据训练双向 LSTM （Bidirectional LSTM, biLSTM）网络，预训练完成后所得到的 biLSTM 可以用来学习上下文感知的单词表示

- 传统序列神经网络的长文本建模能力较弱，并且不容易并行训练，这些缺点限制了早期预训练模型（如 ELMo）的性能

-  Transformer 架构：基于自注意力机制（Self-Attention），对于硬件非常友好，可以通过 GPU 或者 TPU 进行加速训练，这为研发大语言模型提供了可并行优化的神经网络架构

## 大语言模型（Large Language Model, LLM）
- “扩展法则”（Scaling Law）：通过规模扩展 （如增加模型参数规模或数据规模）通常会带来下游任务的模型性能提升

- GPT-3 可以通过“上下文学习”（In-Context Learning, ICL）的方式来利用少样本数据解决下游任务

- 学术界将这些大型预训练语言模型命名为“大语言模型”1 （Large Language Model, LLM）

## 总结
- 首先，早期的统计语言模型主要被用于（或辅助用于）解决一些特定任务，主要以信息检索、文本分类、语音识别等传统任务为主。随后，神经语言模型专注于学习任务无关的语义表征，旨在减少人类特征工程的工作量，可以大范围扩展语言模型可应用的任务。进一步，预训练语言模型加强了语义表征的上下文感知能力，并且可以通过下游任务进行微调，能够有效提升下游任务（主要局限于自然语言处理任务）的性能。随着模型参数、训练数据、计算算力的大规模扩展，最新一代大语言模型的任务求解能力有了显著提升，能够不再依靠下游任务数据的微调进行通用任务的求解。

# 大语言模型的能力特点
#语言模型的发展历程
- 具有较为丰富的世界知识：大语言模型经过超大规模文本数据的预训练后能够学习到较为丰富的世界知识

- 具有较强的通用任务解决能力（通用性）：大语言模型第二个代表性的能力特点是具有较强的通用任务求解能力，基于大规模无标注文本的下一个词元预测任务本质上可以看作一个多任务学习过程

- 具有较好的复杂任务推理能力


-  具有较强的人类指令遵循能力：大语言模型建立了自然语言形式的统一任务解决模式：任务输入与执行结果均通过自然语言进行表达

- 具有较好的人类对齐能力：通过强化学习使得模型进行正确行为的加强以及错误行为的规避，进而建立较好的人类对齐能力。目前很多线上部署的大语言模型应用，都能够有效阻止典型的模型功能滥用行为，一定程度上规避了常见的使用风险

- 具有可拓展的工具使用能力：工具学习实际上就是借鉴了这一思路，通过具有特殊功能的工具来加强大语言模型的能力

