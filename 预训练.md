---
date: 2025-03-26
tags:
---

内容简述：内容包含原始数据的收集、数据预处理、数据词元化、以及预训练过程中的数据调度方法

--------

# 数据来源
----------

- 多语文本
- 科学文本
- 代码

--------

# 数据预处理

-----------------

## 质量过滤
#### 目标
- 直接收集到的文本数据往往掺杂了很多低质量的数据，质量过滤的目标就是去除掉这些低质量数据

#### 方法
##### 1、基于启发式规则的方法
- 基于语种的过滤
- 基于简单统计指标的过滤：例如符号单词比例、评论的点赞数，除了这些统计特征以外，也可以利用困惑度（Perplexity）等文本生成的评估指标来检测和删除表达不自然的句子。
-  基于关键词的过滤：主要是过滤掉一些重复的部分（ HTML 标签、超链接以及各种模板等）

##### 2、基于分类器的方法
原理：
- 选取部分代表性的数据进行质量标注，以此训练出一个精准的文本质量分类器。
- 在选取样本时，可以将维基百科等高质量数据作为正样本，同时从网页中筛选出含有不良内容或低质量数据的样本作为负样本。

实现分类器的方法：
- 轻量级模型（如 FastText 等）：轻量级模型效率较高，但是分类的准确率和精度可能受限于模型能力
- 可微调的预训练语言模型（如 BERT、BART 或者 LLaMA 等）：预训练语言模型可以针对性微调， 但是分类性能的通用性和泛化性仍然有一定的限制
- 闭源大语言模型 API（如 GPT-4、Claude 3）：闭源大语言模型的能力较强， 但是无法灵活针对任务进行适配，而且用于预训练数据清洗需要花费较高的成本。

总结与提示：
- 对于后两种方法来说，除了简单地进行数据过滤，还可以针对性进行数据的改写， 从而使得一些整体质量还不错、但存在局部数据问题的文本仍然可以被保留下来使用。
- 在进行数据清洗时，过滤效率也是我们需要考虑的因素之一。可以使用启发式规则进行初步筛选，再用分类器方法进行进一步过滤，还可以同时使用多个分类器

## 敏感内容过滤
#### 目标
- 除了去除低质量内容，收集到的数据还可能包括不安全内容或隐私信息，需要进一步进行更为细致的过滤和处理

#### 方法
##### 1、过滤有毒内容
原理：采用基于分类器的过滤方法，利用Jigsaw 评论数据集等资源，训练毒性分类器。通过设置合理的阈值， 训练完成的分类器将能够有效识别并过滤掉含有有毒内容的信息。在训练时注意，Dolma 的技术报告 [101] 指出，使用高阈值时去除的数据会过少，语料中未过滤掉的有毒内容会导致模型在下游任务上的性能下降。

##### 2、过滤隐私内容
原理：利用启发式规则方法，识别删除或者掩盖替换相关信息

--------------------------

## 数据去重

#### 出现问题：
- 由于大语言模型具有较强的数据拟合与记忆能力，很容易习得训练数据中的重复模式，可能导致对于这些模式的过度学习。
- 这些数据也可能导致训练过程的不稳定（训练损失震荡），可能导致训练过程崩溃。
- 为了避免数据集污染问题，还需要从预训练数据集中删除在测试集中可能出现的重复或者相关文本，从而防止训练集和测试集之间的重叠。

#### 关键部分：
##### 1、计算粒度
- 去重可以在句子级别、文档级别和数据集级别等多种粒度上进行
- 在文档级别上，现有方法主要依靠单词或 𝑛 元词组的重叠这类表层特征，来衡量文档的重叠比率，进而检测和删除包含相似内容的重复文档
- 在进行去重的过程中，遵循由大到小的过程，先从数据集和文档级别进行去重，然后可以进一步在句子级别实现更为精细的去重

##### 2、 用于去重的匹配方法
- 有精确匹配算法（即每个字符 64 4.2 数据预处理完全相同）或近似匹配算法（基于某种相似性度量）两种去重方法
- 在近似匹配中，可以采用局部敏感哈希（Locality-Sensitive Hashing, LSH）算法，如最小哈希（MinHash） 来实现。
- 可以在不同层面使用不同的去重匹配方法，例如在文档层面采用近似匹配，在句子层面使用精确匹配

------------

## 数据对训练效果的影响

#数据数量的影响

#数据质量的影响

#数据集污染 
- 数据集污染具体指测试集中的数据已然出现在预训练数据或者微调数据中，使得无法进行准确公正的模型性能测评


-----------

# 数据预处理实践
#数据预处理实践

-------------

# 词元化（Tokenization）
#### 目标
- 旨在将原始文本分割成模型可识别和建模的词元序列，作为大语言模型的输入数据。

#### 方法
##### 1、BPE 分词 
- 原理：BPE 算法从一组基本符号（例如字母和边界字符）开始，迭代地寻找语料库中的两个相邻词元， 并将它们替换为新的词元，这一过程被称为合并。合并的选择标准是计算两个连续词元的共现频率，也就是每次迭代中，最频繁出现的一对词元会被选择与合并。合并过程将一直持续达到预定义的词表大小。

##### 2、 WordPiece 分词 
- 与 BPE 类似，WordPiece 分词算法也是从一个小的词汇表开始，其中包括模型使用的特殊词元和初始词汇表。WordPiece 分词算法并不选择最频繁的词对，而是使用公式为每个词对计算分数。
$$
得分 = 词对出现的频率/第一个词出现的频率 × 第二个词出现的频率
$$

##### 3、Unigram 分词 
- 与 BPE 分词和 WordPiece 分词不同，Unigram 分词方法从语料库的一组足够大的字符串或词元初始集合开始，迭代地删除其中的词元，直到达到预期的词表大小。

##### 4、分词器选用
- 为预训练语料专门训练或设计的分词器会更加有效，尤其是对于那些混合了多领域、多语言和多种格式的语料。通常使用 SentencePiece 代码库为预训练语料训练定制化的分词器， 这一代码库支持字节级别的 BPE 分词和 Unigram 分词。

- 训练出高效的分词器需要重点关注的因素：
	1.分词器必须具备无损重构的特性，即其分词结果能够准确无误地还原为原始输入文本。
	2.分词器应具有高压缩率，即在给定文本数据的情况下，经过分词处理后的词元数量应尽可能少，从而实现更为高效的文本编码和存储。具体来说，压缩比可以通过将原始文本的 UTF-8 字节数除以分词器生成的词元数（即每个词元的平均字节数）来计算：
$$
	压缩率 = UTF-8/字节数词元数
$$
	3.对于特定类型的语料，如不同语言或者数学类语料等，可能需要针对性地设计分词器以提升其分词效果。

##### 5、数据调度
###### 关键
- 各个数据源的混合比例
- 各数据源用于训练的顺序（数据课程，Data Curriculum）

###### 数据混合
 - 作用：由于不同数据源与大语言模型某些特定能力的学习具有紧密的联系，因此设置合适的数据混合比例非常重要。在预训练期间，将根据混合比例从不同数据源中采样数据： 数据源的权重越大，从中选择的数据就越多。进一步，可能会对每个数据源的全部数据进行上采样或下采样，以创建特定的数据混合集合作为预训练数据。
 - 数据混合策略：
	1.增加数据源的多样性：增加数据源异质性（即包括多样化的数据源）能够有助于改进大语言模型在下游任务中的综合表现
	2.优化数据混合：除了手动设置数据混合配比外，还可以使用可学习的方法来优化数据组成，以改善模型的预训练效果，通过不断用新的小模型在根据前一次训练结果，增加未较好学习部分的权重的条件下进行训练，并最终获得最理想性能的数据混合配比。
	3.优化特定能力：大语言模型的模型能力在很大程度上取决于数据选择和配比，可以通过增加特定数据源的比例来增强某些对应的模型能力。

###### 数据课程
- 定义：数据课程是指按照特定的顺序安排预训练数据进行模型的训练。
- 作用：相关研究表明，为了学习某些特定的技能，按照技能依赖顺序编排对应数据集的学习方法（例如，基本技能 → 目标技能）比直接在相关的特定语料库上学习效果更好。

---------------

## 数据准备部分总结

##### 一般流程：
- 数据收集：建议在预训练数据中尽量包含较为多样化的数据来源。除了大规模网页数据外，还可以融入多样化的高质量文本，如代码、书籍、科学论文等。

- 数据清洗：整个预处理流程涵盖了质量过滤、去重、隐私去除以及词元化等多个关键环节。

- 数据调度：确定训练大语言模型的数据混合配比以及数据训练顺序。本质上来说，这个过程是在探索数据来源与模型能力之间的潜在关系。为了确定这两种关键策略，一种较为实用的方法是首先使用多个候选策略训练多个小型语言模型，然后从中选择一个最优的训练策略。
