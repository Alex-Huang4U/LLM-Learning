背景：解码策略将主要关注如何让基于概率选择合适的下一个词。自回归解码的策略不局限于特定架构。

#### 常见解码策略
- ##### 贪心算法（Greedy Search）
- ##### 概率采样(Probability Sampling)

#### 贪心搜索改进
- 束搜索：简单来说就是原本是根据每个词的概率大小决定下一个词元，现在改变为根据接下来多个词元的联合概率来确定下个词元
- 长度惩罚（Length Penalty）:简单来说就是进行长度的归一化，使得模型不会倾向于生成更短的句子，也是在鼓励模型生成更长的句子
- 重复惩罚：主要是为了解决贪心搜索重复生成的问题，简单来说，词元出现的次数、频率越大，收到的惩罚越大，被选中的概率越小，此方法也适用于随机采样

#### 随机采样改进策略
- 温度采样：目的是为了增加高概率词元的采样概率，同时降低低概率词元的采样概率
-  Top-𝑘 采样（Top-𝑘 Sampling）. 与温度采样不同，top-𝑘 采样策略是直接剔除概 率较低的词元，限制模型从概率最高的前 𝑘 个词元中进行采样。
-  Top-𝑝 采样（Top-𝑝 Sampling）：简单来说就是设置一个最高总概率阈值，先将所有可能词元按照概率从高到低排列，并从最高开始一个一个将词元放入一个临时集合中，当这些词元的总概率超过阈值时就停止将词元加入集合
- 对比解码：大模型拥有更强的生成能力，有时候会有多个重要词元，而通过于小模型进行对比，就可以知道哪些词元更为重要

------------------------------

#### 解码加速算法
解码存在两个阶段，分别是全量解码和增量解码
- 全量解码阶段，对于输入序列，一次性地计算其状态并缓存键值矩阵
- 增量解码阶段，只计算上一步新生成词元的状态，并不断以自回归的方式生成新词元并更新键值缓存，直到生成结束

#### 模型运行的性能指标
- 运算量
- 访存量：自注意力部分、前馈网络部分，其他部分

#### 系统优化升级
- 目标：减少访存量

- FlashAttention：核心思路是减少注意力计算中的访存量，从而提升计算强度。通过矩阵分块和算子融合的方式，将中间结果一直保留在缓存中，直到获得最终结果后再写回显存中，从而减少了显存读写量

- PagedAttention：针对键值缓存拼接和注意力计算的优化操作，能够有 效降低这两个运算部分的访存量，从而提高计算效率。PagedAttention 引入了操作系统中显存分页管理的方法，预 先将显存划分成若干块给之后的键值缓存“预留空间”，从而显著减少了拼接时反 复分配显存的操作。

- 批次管理优化：将每个输入实例视为一个请求，每个请求的处理过程可以分解为全量解码阶段和若干个单步增量解码阶段。在实现中，连续批处理技术会通过启发式算 法来选择部分请求进行全量解码操作，或者选择一些请求进行单步增量解码操作。 通过这样细粒度的拆分，连续批处理技术在一步操作中能够容纳更多的请求（相 当于提高批次大小），从而提升了计算强度。

#### 解码策略优化
- 推测解码：简单来说就是用小模型往后生成少量词，由大模型验证修改。
- 级联解码：根据解码不同请求的不同难度，分别使用不同规模的模型来处理请求，从而实现最小化解码时间的效果
- 非自回归解码：基于输入，并非一个词元一个词元预测生成，而是一次性生成一组词元，也可以称为半自回归生成。
- 早退机制：在多层transformer模型中，可能不需要经过所有层就能较为可靠的预测下一个词元，故可以设置一个早退判断条件，当分布熵小于预定值，即继续经过网络概率变化小于某个阈值，则判断可以早退。

-------

#### 量化基础知识
- 目标：这是一种将庞大参数量模型压缩的模型量化方法，使得大模型占用显存减少。

##### 量化数学表达
- 量化其实就是将连续的输入映射到离散的输出集合。一般来说这个过程涉及到四舍五入或截断近似等操作

##### 量化策略
- 均匀量化和非均匀量化
	均匀量化是指在量化过程中，量化函数产生的 量化值之间的间距是均匀分布的。这种方法通常用于将连续的数据转换为离散的 表示，以便在计算机中进行高效处理。与此不同，在非均匀量化方法中，它的量化值不一定均匀分布，可以根据输入数据的分布范围而进行调整。其中，均匀量 化方法因其简单和高效的特点而在实际中被广泛使用。
- 对称量化和非对称量化
	对称量化与非 对称量化的一个关键区别在于整数区间零点的映射，对称量化需要确保原始输入 数据中的零点（𝑥 = 0）在量化后仍然对应到整数区间的零点。而非对称量化则不 同，根据前面的公式可以看出此时整数区间的零点对应到输入数值的 𝑆 · 𝑍
- 量化粒度选择
	量化算法通常针对一个批次的数据进行处理，其中批次的 规模大小就反应了量化粒度，可以由算法设计人员进行选择

--------------


#### 大模型训练后的量化方法
- 注：由于大模型使用训练后量化方法消耗的算力较小，所以该方法应用较为广泛

##### 训练后量化方法
- 权重量化
	首先介绍主要面向模型权重的量化方法。其中，主流的权重量化方法通常是 基于逐层量化的方法进行设计的，旨在通过最小化逐层的重构损失来优化模型的 量化权重
- 权重和激活值量化
	- 细粒度量化：对权重量化，可以从每个张量改为对每一 个通道使用一套量化参数。对于激活值量化来说，则是从每个张量改为对每个词 元使用一套量化参数。
	- 混合精度分解：当模型参数规模超过一定阈值后（如 6.7B），神经网络中的激活值中会出现一些异常的超大数值，称为异常值涌现现象。 有趣的是，这些异常值主要分布在 Transformer 层的某些特定激活值特征维度中。将异常值与正常值进行分别量化，异常值以较高精度量化可以恢复这些异常值
	- 量化难度平衡:由于激活值中的异常问题比权重更加明显，所以可以将量化难度从激活值转移到模型权重上。

-----------------------------

#### 其他模型压缩方法

##### 模型蒸馏
- 目标：就昂复杂模型包含的知识迁移到简单模型中，核心思想是引入而外损失函数，训练学生模型的输出尽可能接进教师模型的输出。


##### 模型剪枝
- 目标：在尽可能不损失模型性能的情况下，努 力消减模型的参数数量，最终有效降低模型的显存需求以及算力开销。

----------------------


