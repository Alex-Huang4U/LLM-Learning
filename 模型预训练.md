---
date: 2025-03-31
---

# 预训练任务
- 要求：往往需要设计合适的==自监督预训练任务==，使 得模型能够从海量==无标注数据中==学习到广泛的语义知识与世界知识。
- 常用预训练任务类型：
	1.语言建模（Language Modeling, LM）
	2.去噪自编码（Denoising Autoencoding, DAE）
	3.混合去噪器（Mixture-of-Denoisers, MoD）

------------------

## 语言建模
- 任务核心：预测下一个词元
- 训练本质：一种多任务学习过程
- 变种模型：前缀语言建模（Prefix Language Modeling）
	1.文本序列随机切割成前缀与后缀
	2.仅后缀中的词元损失被计入总损失
	3.未将所有词元都纳入计算

- 中间填充任务
	1.输入被划分成3部分，中间部分被移至末尾，目的是训练填充中间缺失信息的能力
	2.通常作为标准语言建模的辅助，常用于代码预训练模型，从而提升模型的代码补全能力

----------

## 去自噪编码
- ###### 操作以及目标：
	首先将文本进行一系列随机的替换或者删除操作，形成损坏的文本。模型的目标是恢复出被替换或者被删除的词元片段。公式如下：
$$
\begin{equation}
\mathcal{L}_{\text{DAE}}(\boldsymbol{u}) = \log P(\tilde{\boldsymbol{u}}|\boldsymbol{u}_{\backslash\tilde{\boldsymbol{u}}})
\end{equation}
$$
- 由于去噪自编码任务的实现更为复杂，所以需要设定额外的优化策略，如词元替换策略、替换片段长度、替换词元比例等。


----------------

## 混合去噪器
- ###### 别称：UL2
- ###### 操作：
	通过将语言建模和去噪在编码的目标均视为不同类型的去噪任务，对于预训练任务进行了统一建模
- ###### S-去噪器:
	类似前缀语言建模，给前缀以生成合理后缀作为目标
- ###### R-去噪器和X-去噪器：
	与去噪自编码任务的优化目标相似，两者掩盖片段的跨度和损坏比例上有差距，要求模型能够精准还原原始信息。增加了任务难度，迫使模型学习到更全面的文本表示。
- 为了引导模型针对不同类型的输入选择相应的去噪器，输入句子会以特殊词 元（如 [R], [S], [X]）作为开头。模型根据去噪器的类型，对去噪器损坏的词元进行还原。
- ###### 例：UL2和PaLM

------------

# 优化参数设置
- ###### 优化方法：
	通常使用批次梯度下降算法来进行模型参数的调优。通过调整学习率以及优化器中的梯度修正策略，可以进一步提升训练的稳定性。为了防止过拟合，训练中还需要引入一系列正则化方法

---------------

## 基于批次数据的训练
- 在大模型的预训练中，Batch size通常设置为较大数值，从而提高训练的稳定性和吞吐量。
- 为更好地训练大模型，现在更多工作都采用了动态批次调整策略，即在训练中逐渐增大批次。
- 使用动态批次调整策略的原因：这 是因为较小的批次对应反向传播的频率更高，训练早期可以使用少量的数据让模 型的损失尽快下降；而较大的批次可以在后期让模型的损失下降地更加稳定，使 模型更好地收敛

------------

## 学习率
- 学习率调整策略包含两个不同阶段，预热阶段和衰减阶段，预热阶段一般占整个训练步骤的0.1%至0.5%，然后学习率便开始衰减。
- 先增再减是由于开始时参数是随机的，此时梯度通常较大，要用较小的学习率保证训练的平稳。通常采用线性预热策略来逐步调整学习率。
- 学习率逐渐上升最后会到达一个预设的最大阈值，达到最大阈值之后会逐渐衰减，以避免参数值在较优点附近来回震荡

----------

## 优化器

- ###### 常用优化器：Adam、AdamW
- ###### 操作：
	1.使用梯度的“动量”作为参数的更新方向，它使用历史 更新步骤中的梯度加权平均值来代替当前时刻的梯度，从而缓解样本随机性带来 的损失震荡
	2.Adam 使用自适应的学习率方法，通过梯度的加权“二阶矩” 对梯度进行修正（可以看做使用“标准差”进行“归一化”），从而防止梯度过小 导致模型难以优化

---------

## 稳定优化技术
- ###### 目标：解决训练不稳定问题
- ###### 梯度裁剪.
	用于解决损失突增的问题，具体操作是将梯度限制在一个较小的区间内：当梯度模长超过给定的阈值之后，就按照这个阈值进行截断
- ###### 训练恢复
	每固定步数设置一些模型存档点，当发生异常时选择前一个存档点重启训练，并跳过可能导致问题的数据
- ###### 权重衰减
	利用正则化技术稳定训练过程，在每次更新模型参数时引入衰减系数
- ###### dropout
	该技术在训练中随机将一些神经元的输出置零来避免过拟合，但是在大模型的训练数据和模型中存在归一化结构，已有工程很少使用到该技术

-------------

## 可拓展的训练技术
- ###### 目标：在有限的计算资源下高效地训练模型
- ###### 问题：
	1.如何提高训练效率
	2.如何将庞大的模型有效地加载到不同的处理器中
	
-------------

## 3D并行训练
- 含义：3D 并行策略实际上是三种常用的并行训练技术的组合，即数据并行（Data Parallelism）、流水线并行（Pipeline Parallelism）和张量并行（Tensor Parallelism）
- ###### 数据并行
	1.这是一种提高训练吞吐量的方法
	2.将模型参数和优化器状态复制到多个GPU上，并将训练数据平均分配到这些GPU上
	3.每个GPU只需处理自己的数据然后执行向前传播和反向传播以获取梯度
	4.所有GPU都执行完后，将不同GPU的梯度进行平均得到整体梯度，来统一更新所有GPU上的模型参数
- ###### 流水线并行
	1.流水线并行旨在将大语言模型不同层的参数分配到不同的GPU 上，可以将 Transformer 连续的层加载到同一 GPU 上，以减少 GPU 之间传输隐藏状态或梯度的成本
	2.流水 线并行通常需要配合梯度累积（Gradient Accumulation）技术进行优化，该技术的 主要思想是，计算一个批次的梯度后不立刻更新模型参数，而是累积几个批次后 再更新，这样便可以在不增加显存消耗的情况下模拟更大的批次
- ###### 张量并行
	1.张量并行的分配粒度更细，它进一步分解了模型的参数张量（即参数矩阵），以便 更高效地利用多个 GPU 的并行计算能力
	2.具体地，对于大语言模型中的某个矩阵 乘法操作 𝑾 𝑯，参数矩阵 𝑾 可以按列分成两个子矩阵 𝑾1 和 𝑾2，进而原式可以 表示为 [𝑾1 𝑯, 𝑾2 𝑯]。然后，可以将参数矩阵 𝑾1 和 𝑾2 放置在两张不同的 GPU 上，然后并行地执行两个矩阵乘法操作，最后通过跨 GPU 通信将两个 GPU 的输 出组合成最终结果。常见的张量并行策略是分解模型注意力层的 𝑾𝑄，𝑾𝐾 ，𝑾𝑉 ， 𝑾𝑂 矩阵参数（公式 5.2）和前馈网络层的 𝑾𝑈 ，𝑾𝐷 矩阵参数
	
----------

## 零冗余优化器
- ###### 问题：
	对于每个 GPU，在模型传播到某一层 时，其他层的模型和优化器参数并不参数计算，这导致了严重的显存冗余现象，同 时也限制了每个 GPU 可以支持的前向传播数据量，降低了训练效率。
- ###### 解决方式：
	ZeRO 技术仅在每个 GPU 上保留部分模型参数和优化器参数，当需要时再从其它 GPU 中读取（简单来说就是把模型参数分部分存在了不同的卡中）

--------

## 激活重计算
- ###### 问题：
	给定一个待优 化函数 𝒀 = 𝑿𝑾，在反向传播时需要 𝑿 的值才能计算 𝑾 的导数，所以在前向传播 时需要保留这些 𝑿（通常被称为激活值）。然而，保存每一层所有的激活值需要占用大量的显存资源
- ###### 解决方案：
	激活重计算技术在前 向传播期间仅保留部分的激活值，然后在反向传播时重新计算这些激活值，以达 到节约显存的目的，但是同时也会引入额外的计算开销

-------------

## 混合精度训练
- ###### 操作：
	通过同时使用半精度浮点 数（2 个字节）和单精度浮点数（4 个字节）进行运算，以实现显存开销减半、训练 效率翻倍的效果。具体来说，为了保证表示精度，需要保留原始 32 位模型的参数 副本。但在训练过程中，会先将这些 32 位参数转换为 16 位参数，随后以 16 位精 度执行前向传播和反向传播等操作，最后在参数更新时再对 32 位模型进行优化。 由于在模型训练中前向传播和反向传播占用了绝大部分优化时间，混合精度训练 因而能够显著提升模型的训练效率。
- ###### 常见精度
	1.FP16，其包含 1 位符号位、5 位指数位和 10 位尾数位，表示范围为 −65504 到 65504
	2.BF16，其包含 1 位符号位、8 位指数位和 7 位尾数位，表示范围可以达到 1038 数量级
	