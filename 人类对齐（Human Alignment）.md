## 人类对齐的背景与标准
- 背景：尽管大模型在下游任务中表现优秀，但是仍然有错误或者具有危害性的行为，在大语言模型的预训练和有监督微调当中1，主要是根据上一个词元对下一个词元进行预测，这个过程并未充分考虑到人类的价值观或者偏好，进而导致产生一些人类不喜欢的生成模式。

-----------------

## 对齐标准
- ###### 有用性：
	能够提供有用信息，能够准确完成任务，正确理解上下文，并且展现出一定的创造性与多样性。应具备主动获取任务相关信息的能力。

- ###### 诚实性：
	大模型输出应当客观真实，在输出时应当适当表达出信息的不确定性程度。本质上，这要求模型理解自身的能力和知识水平。

- ###### 无害性：
	在处理敏感主题时应当有特殊的处理方式，消除冒犯性和歧视性。同时模型需要能够检测到具有恶意目的查询请求，当被诱导执行恶意行为时，应直接拒绝。


--------------------

## 基于人类反馈的强化学习

### RLHF(Reinforcement Learning from Human Feedback)
- 目标：利用收集到的人类反馈数据指导大模型进行微调，实现人类对齐
- 操作：首先收集人类对于大模型不同输出的偏好，然后使用收集到的人类反馈数据训练奖励模型，最后基于奖励模型使用强化学习算法微调大语言模型

#### RLHF算法系统
- RHLF算法系统的三个关键组成部分：
	1.需要与人类价值观对齐的模型
	2.基于人类反馈数据学习的奖励模型
	3.用于训练大模型的强化学习算法

##### RLHF关键步骤
- 监督微调：指令数据一般包括任务描述和示例输出，并按照先前已经介绍过的[[指令微调]]步骤进行微调
- 奖励模型训练：使用人类反馈数据训练奖励模型。首先使用语言模型针对任务指令生成一些列的候选输出，然后对输出文本进行偏好标注，进一步用标注好的数据进行奖励模型的训练。在 InstructGPT 中，标注员将模型生成的输出按照最佳到最 差的顺序进行排序，并据此训练奖励模型来预测这个排序。
- 强化学习训练：在这一步骤中，语言对齐被转化为一个强化学习问题。待对齐语言模型担任策略实施者的角色（称为策略模型），接受输入返回输出，奖励模型根据当前的输出提供相应的奖励分数，指导策略模型的优化。为了避免当前训练轮次的语言模型明显偏离初始（强 化学习训练之前）的语言模型，通常会在原始优化目标中加入一个惩罚项（如 KL 散度）。由于强化学习算法的不稳定性，学术界提出了一些采用加暗渡微调的对其算法。

------------------------

### 人类反馈数据的收集
- 反馈数据的收集形式主要有
	1.基于评分的人类反馈：简单来说就是设计一些评分规则，并由人类判断大模型的输出是否违背了这些标准，并对此进行打分
	2.基于排序的人类反馈
	使用游戏中常用的Elo机制，通过对模型输出进行两两对比，获得胜利的输出分级就会上升、反之下降。其中，上升下降的幅度取决于估计的胜负概率以及实际的获胜情况，如果两者差距较大，则上升或者下降的幅度也随之增大。

-------------------

## 奖励模型的训练方法
- 目标：奖励模型的出现是为了替代人类在RLHF训练中为模型的输出提供实时反馈，经过充分训练的奖励模型能够有效的拟合人类的偏好在强化学习的过程中替代人类提供反馈信号。

#### 训练方法（与上述人类反馈数据的收集方式相对应）
	奖励模型是基于语言模型设计的，将输出从一个词向量转化为一个分数的标量
- 打分式：参考人类的打分对输入进行打分，对输出进行损失计算
- 对比式：两两进行对比，参考人类对输入进行正例加分以及降低负例分数，这样使得奖励模型能够有效的学习区分正例和负例
- 排序式：可以视为对比式的一种增强形式，对于给定输入，人类根据模型的多个输出进行排序，而基于这样的方式进行学习的模型，在一定程度上学习到更为全局的排序关系

#### 训练策略
	为了进一步增强奖励模型对于人类偏好的拟合能力，可以通过修改训练过程 的目标函数、选取合适的基座模型和设置合理的奖励计算形式等方式来优化奖励 模型的训练过程
- 目标函数优化
	在训练大规模奖励模型时，有时会遇到过拟合问题。为了解 决这一问题，可以将最佳的模型输出所对应的语言模型损失作为正则项，从而缓 解奖励模型在二元分类任务上的过拟合问题。奖励模型在学习最大化正负例分数差距的同时，学习如何根据输入生成正例
- 基座模型选取
	使用更大的奖励模型通常能更好的判断模型的输出质量，提供更准确的反馈信号。使用相同的检查点来初始化待对齐语言模型和奖励模型，由于奖励模型 与待对齐模型拥有相同的预训练知识，这一方法可以有效地减少两者之间的信息 不匹配问题，加强模型对齐效果。
- 奖励计算形式
	由于对齐存在多个标准，单一奖励模型难以满足所有对其标准。因此可以根据不同对齐标准训练多个奖励模型，然后使用组合策略如加权平均等方式计算基于这些奖励模型的最终奖励。同时可以对不同的标准定义不同的权重，例如，在有用性方面可以适当 放松要求，但对有害性施加更严格的限制。

------------------

## 强化学习训练
- 强化学习的目标就是通过学习合适的策略，后获得外部的最大奖励
- 在智能体和外部环境第 𝑡 次交互的过程中， 智能体需要根据当前外部环境的状态 𝑠𝑡 选择合适的策略，决定下一步该做出的行 动 𝑎𝑡 。当智能体采取了某个行动之后，外部环境会从原来的状态 𝑠𝑡 变化为新的 状态 𝑠𝑡+1。此时，外部环境会给予智能体一个奖励分数 𝑟𝑡 
- 在自然语言生成任务中，大语言模型（即策略模型）需要根据用户输入的问题 和已经生成的内容（即当前状态），生成下一个词元（即对下一步行动做出决策）。当大语言模型完整生成整个回复之后（即决策轨迹），标注人员（或奖励模型）会.针对大语言模型生成的回复进行偏好打分（即奖励分数）,大语言模型需要学习生 成回应的有效策略，使得生成的内容能获得尽可能高的奖励，即其生成的内容尽 可能符合人类的价值观和偏好。
- 策略梯度算法：策略模型和外部环境进行交互，并使用交互得到的数据对策略模型的参数进行优化，这是一种在线策略的训练方式（On-policy）。基于在 线策略的训练方法为了保证采样得到的策略轨迹能够近似策略模型做出的决策的 期望，需要在每次调整策略模型参数之后重新进行采样。
- 与策略梯度算法不同，近端策略优化使用了离线策 略（Off-policy）的训练方式，即训练过程中负责交互与负责学习的策略模型不同。 也就是说，负责学习的策略模型通过另一个模型与环境交互产生的轨迹进行优化。 使用离线策略的训练方法，由于采样的模型是固定的，所以同一批数据可以对负 责学习的策略模型进行多次优化，以提升数据的使用效率，使训练过程更为稳定

### PPO介绍
- ###### 功能简述：
	近端策略优化模型是主要用于训练能够根据外部环境状态做出行为决策的策略模型。其在策略梯度算法的基础上，主要使用优势估计来更加准确的评估决策轨迹能获得的奖励，使用了重要性采样来进行离线策略训练。此外，为了保证重要 性采样的稳定性，PPO 算法通过在目标函数中加入了梯度裁剪以及相关的惩罚项 来减小采样误差。为了能够实现上述优化过程，PPO 在策略模型和奖励模型的基 础上，还引入了参考模型和评价模型。
- ###### 关键步骤：
	1.优势估计：在 PPO 的优势函数中，通过将决策的奖励与期望奖励做差，产生较低奖励的 决策将会得到一个负的优势值，而产生较高奖励的决策会得到一个正的优势值。这 些相对较差的决策就会被抑制，同时鼓励策略模型产生收益更高的决策。因此，优 势函数可以帮助策略模型学习在众多决策中做出更好的选择。
	
	2.重要性采样：本质就是通过对旧数据进行适当的加权，使得模型能够更好的处理新数据
	
	3。基于梯度裁剪的目标函数：通过裁剪策略比率的变化范围，防止策略更新过于激进。主要保证了新旧策略模型产生的决策差距不会过大，保证了重要性采样算法的稳定性。
	
	4.基于 KL 散度的目标函数：𝛽 是一个超参数，在策略模型的优化过程中针对训练情况可以进行动态调 整。当 KL 散度的值较小时，适当调小 𝛽 的取值，策略模型可以针对性的更新参 数以产生更好的策略；当 KL 散度的值较大的时候，适当调大 𝛽 的取值，从而减 少策略模型的更新程度

-------------------
